__The following are the five key problems that represent the vast performance problems in Apache Spark__

- Skew : An imbalance in the size of partitions across all datasets 
- Spill : The writing of temp files to disk due to lack of memory for processing data
- Shuffle : The movement of data across executors due to wide transformation
- Storage : A set of problems directly related to how data is stored on disk
- Serialization : Distribution of code segments across the cluster

`sc.setJobDescription("Step A-S: Basic Initialization")` --> Used to define spark job sent to the executors



