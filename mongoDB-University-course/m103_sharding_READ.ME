SHARDING
---------------------------------------------------------------------------------------------------------
Configuration file for first config server:

sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26001
systemLog:
  destination: file
  path: /var/mongodb/db/csrs1.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs1

Configuration file for 2nd config server:

  sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26002
systemLog:
  destination: file
  path: /var/mongodb/db/csrs2.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs2

Configuration file for 3rd config server:

sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26003
systemLog:
  destination: file
  path: /var/mongodb/db/csrs3.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs3

Starting the 3 config servers:

mongod -f csrs_1.conf
mongod -f csrs_2.conf
mongod -f csrs_3.conf

Connecting to one of the config servers

mongo --port 26001

Initiating the CSRS 

rs.initiate()

Creating superuser on CSRS:

use admin
db.createUser({
  user: "m103-admin",
  pwd: "m103-pass",
  roles: [
    {role: "root", db: "admin"}
  ]
})

Authenticaing as superuser:

db.auth("m103-admin", "m103-pass")

Add the 2nd and 3rd node to the CSRS:

rs.add("192.168.103.100:26002")
rs.add("192.168.103.100:26003")

mongos Config file:

sharding:
  configDB: m103-csrs/192.168.103.100:26001,192.168.103.100:26002,192.168.103.100:26003
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26000
systemLog:
  destination: file
  path: /var/mongodb/db/mongos.log
  logAppend: true
processManagement:
  fork: true

Start the mongos server:

mongos -f mongos.conf

Connect to the mongos:

mongo --port 27000 --username m103-admin --password m103-pass --authenticationDatabase admin

Check sharding status:

sh.status()

Updated config for node1.conf:

sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node1
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27011
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node1/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-repl

Updated config for node2.conf:

sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node2
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27012
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node2/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-repl

Updated config for node3.conf:

sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node3
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27013
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node3/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-repl

Connecting directly to secondary node(note that if an election has taken place in your replica set, 
the specified node may have become primary):

mongo --port 27012 -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

Shutting down node:

use admin
db.shutdownServer()

Restarting node2 with new configuration:

mongod -f node2.conf

Stepping down the current primary:

rs.stepDown()

Adding then new shard to cluster from mongos 

sh.addShard("m103-repl/192.168.103.100:27012")
-----------------------------------------------------------------------------------------------------------------------
Config DB 
----------------------------------------
This database should not be written into. However, there are important info we are gonna get as shown below:
- Shards - Chunk - mongos 
--------------------------------------------------------------------------------------------------
SHARD KEY 
----------------
This provides inclusive lower bound and exclusive upper bound of the use to distribute data 
in a mongodb sharded cluster. Shard key has the following features:
-- Shard key must be present in every document in a collection. 
-- Shard key must be indexed.
-- Shard key are immutable(sharded keys and its values cannot be changed)
-- Shard keys are permanent(a sharded collection can't be unsharded)

HOW TO SHARD?
-----------------------------------------
1. sh.enableSharding(<database>)
2. db.collection.createIndex()
3. sh.shardCollection("<database>.<collection>", { shard key }) to shard collection.

WHAT MAKES A GOOD SHARD KEY?
------------------------------------------
1. High cardinality --> many possible unique values 
2. Low frequency --> very little repetition of unique sharded key value 
3 Avoid monotically increasing sharded key

mongoimport --host localhost:26000 -u m103-admin -p m103-pass --db m103 --collection products --file /dataset/products.json --authenticationDatabase admin

mongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin
-------------------------------------------------------------------------------------------------------------------
To change chunksize in the config database 

use config
db.settings.save({_id: "chunksize", value: 2})

----------------------------------------------------------------------------------------------------------------
BALANCER MANAGEMENT METHODS 
--------------------------------------------------------------------------------
1. sh.startBalancer(timeout, interval)
2. sh.stopBalancer(timeout, interval)
3. sh.setBalancerState(boolean)

timeout --> Time required to start/stop the balancer. 
interval --> Defines how long the client should wait before checking the balancer status again




